# -*- coding: utf-8 -*-
"""Model1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kl85c4t6mTjDfKYpnIquUIQKreJ9-LCY
"""

import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')
nltk.download('stopwords')

text="""Hello Mr. Dhwanil, How are you doing today? Hope you are doing good and weather is awesome today.
It's a good day for hiking and we should do some hiking.You are bullshit."""

from nltk.tokenize import sent_tokenize
tokenized_text=sent_tokenize(text)
print(tokenized_text)

from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
print(tokenized_word)

Lowercase = []
for lowercase in tokenized_word:
    Lowercase.append(lowercase.lower())
print(Lowercase)

from nltk.probability import FreqDist
fdist = FreqDist(tokenized_word)
print(fdist)

fdist.most_common(2)

import matplotlib.pyplot as plt
fdist.plot(30,cumulative=False)
plt.show()

from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))
print(stop_words)

filtered_sent=[]
for w in tokenized_word:
    if w not in stop_words:
        filtered_sent.append(w)
print("Tokenized Sentence:",tokenized_word)
print("Filterd Sentence:",filtered_sent)

# Stemming
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize

ps = PorterStemmer()

stemmed_words=[]
for w in filtered_sent:
    stemmed_words.append(ps.stem(w))

print("Filtered Sentence:",filtered_sent)
print("Stemmed Sentence:",stemmed_words)

pip install profanity-filter

import spacy
from profanity_filter import ProfanityFilter

nlp = spacy.load('en')

profanity_filter = ProfanityFilter(nlps={'en': nlp})
nlp.add_pipe(profanity_filter.spacy_component, last=True)

doc = nlp("'It's here! The new AMP63 pistol is now available via in-game challenge and Store bundle.  Let's do this. You bullshit.'")
doc

doc._.is_profane

for token in doc:
  if token._.is_profane: 
    print(f'{token}: '
          f'censored={token._.censored}, '
          "\n"f'is_profane={token._.is_profane}, '
          f'original_profane_word={token._.original_profane_word}'
    )



